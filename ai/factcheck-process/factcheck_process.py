from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from transformers import (
    BartForConditionalGeneration, PreTrainedTokenizerFast,
    AutoModelForSequenceClassification, AutoTokenizer
)

app = FastAPI()

# âœ… 1. KoBART ëª¨ë¸ ë¡œë“œ (í…ìŠ¤íŠ¸ ìš”ì•½)
kobart_model = BartForConditionalGeneration.from_pretrained("digit82/kobart-summarization")
kobart_tokenizer = PreTrainedTokenizerFast.from_pretrained("digit82/kobart-summarization")

# âœ… 2. KLUE-RoBERTa ëª¨ë¸ ë¡œë“œ (ìì—°ì–´ ì¶”ë¡ , "klue/roberta-base" ì‚¬ìš©)
nli_model = AutoModelForSequenceClassification.from_pretrained("klue/roberta-base")
nli_tokenizer = AutoTokenizer.from_pretrained("klue/roberta-base")

class FactCheckRequest(BaseModel):
    title: str
    content: str

@app.post("/factcheck")
async def fact_check(request: FactCheckRequest):
    try:
        title = request.title
        content = request.content

        # âœ… 1. ë‰´ìŠ¤ ë³¸ë¬¸ ìš”ì•½
        inputs = kobart_tokenizer.encode("summarize: " + content, return_tensors="pt", max_length=512, truncation=True)
        summary_ids = kobart_model.generate(inputs, max_length=150, num_beams=4, early_stopping=True)
        summary = kobart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

        print("ğŸ”¹ [ë©”ì¸ ë‰´ìŠ¤ íƒ€ì´í‹€] :", title)
        print("ğŸ”¹ [ìš”ì•½ëœ ë‚´ìš©] :", summary)

        # âœ… 2. ìì—°ì–´ ì¶”ë¡  (íŒ©íŠ¸ ê²€ì¦)
        premise = summary  # ìš”ì•½ëœ ë³¸ë¬¸ (ì „ì œ)
        hypothesis = title  # íŒ©íŠ¸ì²´í¬ ì œëª© (ê°€ì„¤)

        # ğŸ”¹ ë¶€ì •í˜• ì œëª©ì¸ ê²½ìš° ë°˜ëŒ€ ë¬¸ì¥ì„ ì¶”ê°€
        if any(neg_word in title for neg_word in ["ì—†ë‹¤", "ì•„ë‹ˆë‹¤", "ì•„ë‹Œ"]):
            reversed_hypothesis = title.replace("ì—†ë‹¤", "ìˆë‹¤").replace("ì•„ë‹ˆë‹¤", "ì´ë‹¤").replace("ì•„ë‹Œ", "ì¸")
            hypothesis += " / " + reversed_hypothesis  # ì›ë˜ ë¬¸ì¥ + ë°˜ëŒ€ ë¬¸ì¥ í•¨ê»˜ ì‚¬ìš©

        encoded_input = nli_tokenizer(premise, hypothesis, return_tensors="pt", truncation=True, padding=True)
        output = nli_model(**encoded_input)
        logits = output.logits
        probs = torch.nn.functional.softmax(logits, dim=1)

        # âœ… 3. ì¶œë ¥ í¬ê¸° í™•ì¸ í›„ ì•ˆì „í•œ ì ‘ê·¼
        num_classes = probs.shape[1]  # ëª¨ë¸ì˜ ì‹¤ì œ ì¶œë ¥ í¬ê¸°
        entailment_prob = probs[0][0].item()  # True í™•ë¥ 

        if num_classes == 3:
            contradiction_prob = probs[0][2].item()  # False í™•ë¥  (3-class ì§€ì›í•˜ëŠ” ê²½ìš°)
        else:
            contradiction_prob = 1 - entailment_prob  # 2-class ëª¨ë¸ì¸ ê²½ìš° ë³´ì™„

        # âœ… 4. í™•ë¥ ê°’ ê¸°ë°˜ íŒë³„ ë¡œì§ ê°œì„ 
        if contradiction_prob > 0.5:
            result = "False"
        elif entailment_prob > 0.5:
            result = "True"
        else:
            result = "Partially True" 

        print(f"ğŸ”¹ [ê²€ì¦ ê²°ê³¼] : {result} (Entailment: {entailment_prob:.2f}, Contradiction: {contradiction_prob:.2f})")

        return {"title": title, "summary": summary, "factcheck_result": result}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
