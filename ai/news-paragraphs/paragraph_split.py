from transformers import BertTokenizer, BertModel
import torch
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# âœ… Sentence-BERT ëª¨ë¸ ë¡œë“œ
sbert_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

def split_sentences_by_period(text):
    """
    ë¬¸ì¥ì„ ì˜¨ì ('.')ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ  ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    """
    sentences = [s.strip() for s in text.split('.') if s.strip()]
    return [s + '.' for s in sentences]  # ê° ë¬¸ì¥ ëì— '.'ì„ ìœ ì§€

def get_sentence_embeddings(text):
    """
    KoBERTë¥¼ í™œìš©í•˜ì—¬ ë¬¸ì¥ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ì–»ìŒ.
    """
    tokenizer = BertTokenizer.from_pretrained("monologg/kobert")
    model = BertModel.from_pretrained("monologg/kobert")

    sentences = split_sentences_by_period(text)  # ğŸ”¹ ì˜¨ì  ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë¶„ë¦¬
    embeddings = []

    for sent in sentences:
        tokens = tokenizer(sent, return_tensors="pt", padding=True, truncation=True)
        with torch.no_grad():
            outputs = model(**tokens)
        embeddings.append(outputs.last_hidden_state.mean(dim=1).numpy())

    return sentences, np.array(embeddings).squeeze()

def split_paragraphs_bert(text, threshold=0.9):
    """
    ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ì—¬ ë¬¸ë‹¨ì„ êµ¬ë¶„.
    """
    sentences, embeddings = get_sentence_embeddings(text)
    similarities = cosine_similarity(embeddings)

    paragraphs = []
    current_paragraph = [sentences[0]]

    for i in range(1, len(sentences)):
        if similarities[i - 1][i] < threshold:  # ìœ ì‚¬ë„ê°€ ë‚®ìœ¼ë©´ ìƒˆë¡œìš´ ë¬¸ë‹¨ ì‹œì‘
            paragraphs.append(" ".join(current_paragraph))
            current_paragraph = [sentences[i]]
        else:
            current_paragraph.append(sentences[i])

    if current_paragraph:
        paragraphs.append(" ".join(current_paragraph))

    return paragraphs

def merge_short_paragraphs(paragraphs, min_length=10):
    """
    ë„ˆë¬´ ì§§ì€ ë¬¸ë‹¨ì„ ì´ì „ ë¬¸ë‹¨ê³¼ í•©ì¹˜ëŠ” í•¨ìˆ˜
    """
    fixed_paragraphs = []
    current_paragraph = ""

    for paragraph in paragraphs:
        if len(paragraph) < min_length and fixed_paragraphs:
            # í˜„ì¬ ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì´ì „ ë¬¸ì¥ê³¼ í•©ì¹¨
            fixed_paragraphs[-1] += " " + paragraph
        else:
            fixed_paragraphs.append(paragraph)

    return fixed_paragraphs

def merge_based_on_similarity(paragraphs, threshold):
    """
    ë¬¸ë§¥ì´ ìœ ì‚¬í•œ ë¬¸ë‹¨ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ëŠ” í•¨ìˆ˜ (Sentence-BERT ê¸°ë°˜)
    """
    if len(paragraphs) < 2:
        return paragraphs  # ë³‘í•©í•  ë¬¸ë‹¨ì´ í•˜ë‚˜ë¿ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

    vectors = sbert_model.encode(paragraphs)  # âœ… SBERT ë¬¸ì¥ ì„ë² ë”© ìƒì„±

    merged_paragraphs = [paragraphs[0]]

    # âœ… ì—°ê²°ì–´ ë¦¬ìŠ¤íŠ¸ (ë¬¸ë‹¨ ì‹œì‘ ì‹œ ë³‘í•© ê¸°ì¤€)
    connection_words = {"íŠ¹íˆ", "ê·¸ëŸ¬ë‚˜", "ê²Œë‹¤ê°€", "ë˜í•œ", "ì´ì™€ í•¨ê»˜", "í•œí¸", "ë”ë¶ˆì–´", "ì´ì— ë”°ë¼", "ê²°ê³¼ì ìœ¼ë¡œ", "ì¦‰", "ì´ëŠ”"}

    for i in range(1, len(paragraphs)):
        similarity = cosine_similarity([vectors[i - 1]], [vectors[i]])[0][0]

        # âœ… ìœ ì‚¬ë„ê°€ ë†’ê±°ë‚˜, ë¬¸ì¥ì´ ì—°ê²°ì–´ë¡œ ì‹œì‘í•˜ë©´ ë³‘í•©
        if similarity > threshold or paragraphs[i].split()[0] in connection_words:
            merged_paragraphs[-1] += " " + paragraphs[i]
        else:
            merged_paragraphs.append(paragraphs[i])

    return merged_paragraphs

if __name__ == "__main__":
    sample_news = """
    ì¹´ì¹´ì˜¤í†¡, ì¹´ë‚˜ë‚˜ ë“± ì„œë¹„ìŠ¤ì— ì˜¤í”ˆAI ê¸°ìˆ  ì ìš© ì˜ˆì •.
ìƒ˜ ì˜¬íŠ¸ë¨¼ ì˜¤í”ˆAI CEOë„ ì°¸ì„...ì „ëµì  ì œíœ´ ì²´ê²° ë°œí‘œ.
ì •ì‹ ì•„ ì¹´ì¹´ì˜¤ ëŒ€í‘œê°€ 4ì¼ ì˜¤ì „ ì„œìš¸ ì¤‘êµ¬ ë” í”Œë¼ì í˜¸í…” ì„œìš¸ì—ì„œ ì—´ë¦° ê¸°ìê°„ë‹´íšŒì— ìƒ˜ ì˜¬íŠ¸ë¨¼ ì˜¤í”ˆAI CEOì™€ í•¨ê»˜ ì°¸ì„í•˜ê³  ìˆë‹¤. [ì‚¬ì§„ ì—°í•©ë‰´ìŠ¤].
ì¹´ì¹´ì˜¤ê°€ 4ì¼ ì„œìš¸ ì¤‘êµ¬ ë”í”Œë¼ìì—ì„œ ì˜¤í”ˆAI(OpenAI)ì™€ ì „ëµì  ì œíœ´ ì²´ê²°ì— ëŒ€í•œ ê³µë™ ê¸°ìê°„ë‹´íšŒë¥¼ ì—´ì—ˆë‹¤. ì •ì‹ ì•„ ì¹´ì¹´ì˜¤ ëŒ€í‘œëŠ” ì¹´ì¹´ì˜¤ê°€ ê¿ˆê¾¸ëŠ” AI ë¯¸ë˜ì‹œëŒ€ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ë©°, ì¹´ì¹´ì˜¤ê°€ ì§€í–¥í•˜ëŠ” AI ì „ëµ ë°©í–¥ì„ ì„¤ëª…í–ˆë‹¤. ì´ ë‚  ê°„ë‹´íšŒì—ëŠ” ì •ì‹ ì•„ ì¹´ì¹´ì˜¤ ëŒ€í‘œë¥¼ ë¹„ë¡¯í•´ ìƒ˜ ì˜¬íŠ¸ë¨¼ ì˜¤í”ˆAI ìµœê³ ê²½ì˜ì(CEO)ê°€ ì§ì ‘ ì°¸ì„í–ˆë‹¤.
ì¹´ì¹´ì˜¤ì˜ AI ì‚¬ì—… ë°©í–¥ì„±ì€ ì§ì ‘ ê°œë°œë³´ë‹¤ ì´ë¯¸ ê°œë°œëœ AI ëª¨ë¸ ì¤‘ ìµœê³ ì˜ ëª¨ë¸ì„ ê°€ì ¸ì™€ ì ìš©í•˜ëŠ” ê²ƒ. ì¹´ì¹´ì˜¤ê°€ ì°¾ì€ í˜‘ë ¥ì‚¬ëŠ” ì±—GPT ê°œë°œì‚¬ì¸ ì˜¤í”ˆAIë¡œ, êµ­ë‚´ì—ì„œëŠ” ì²˜ìŒìœ¼ë¡œ ì˜¤í”ˆAIì™€ì˜ ì „ëµì  ì œíœ´ë¥¼ ë§ºì—ˆë‹¤.
ì–‘ì‚¬ëŠ” ì§€ë‚œí•´ 9ì›”ë¶€í„° ê´€ë ¨ ë…¼ì˜ë¥¼ ì§„í–‰í•´ì™”ê³ , 'AI ë³´í¸í™”' 'AI ëŒ€ì¤‘í™”'ë¼ëŠ” ìŸì ì—ì„œ ë‘ ê¸°ì—…ì˜ ì§€í–¥ì ì´ ì¼ì¹˜í•´ ì‹¤ì§ˆì ì¸ í˜‘ë ¥ê¹Œì§€ ì´ì–´ì¡Œë‹¤. íŠ¹íˆ ì¹´ì¹´ì˜¤ëŠ” 5000ë§Œëª… ëŒ€í•œë¯¼êµ­ êµ­ë¯¼ì´ ì‚¬ìš©í•˜ê³  ìˆëŠ” 'êµ­ë¯¼ ì±„íŒ…ì•±'ì´ë¼ëŠ” ë¶€ë¶„ì—ì„œ ì˜¤í”ˆAIì˜ ì‚¬ìš©ì í™•ì¥ ëª©í‘œë¥¼ ì±„ìš¸ ê²ƒìœ¼ë¡œ í‰ê°€ëœë‹¤.
ë°œì–¸í•˜ëŠ” ì •ì‹ ì•„ ì¹´ì¹´ì˜¤ ëŒ€í‘œ. [ì‚¬ì§„ ì—°í•©ë‰´ìŠ¤].
ì •ì‹ ì•„ ì¹´ì¹´ì˜¤ ëŒ€í‘œëŠ” ì´ë²ˆ ë°œí‘œì—ì„œ â€œì˜¤ëœ ê¸°ê°„ êµ­ë¯¼ ë‹¤ìˆ˜ì˜ ì¼ìƒì„ í•¨ê»˜ í•˜ë©° ì¶•ì í•´ ì˜¨ ì—­ëŸ‰ì„ ë°”íƒ•ìœ¼ë¡œ â€˜ì´ìš©ìë¥¼ ê°€ì¥ ì˜ ì´í•´í•˜ëŠ” ê°œì¸í™”ëœ AIâ€™ë¥¼ ì„ ë³´ì´ëŠ” ê²ƒì´ ì§€ê¸ˆ ì‹œëŒ€ ì¹´ì¹´ì˜¤ì˜ ì—­í• ì¼ ê²ƒâ€ì´ë¼ë©° â€œê¸€ë¡œë²Œ ê¸°ìˆ  ê²½ìŸë ¥ì„ ë³´ìœ í•œ ì˜¤í”ˆAIì™€ í˜‘ë ¥í•´ í˜ì‹ ì  ê³ ê°ê²½í—˜ì„ ì œê³µí•¨ìœ¼ë¡œì¨ AI ì„œë¹„ìŠ¤ì˜ ëŒ€ì¤‘í™”ë¥¼ ì´ëŒê² ë‹¤â€ê³  ë§í–ˆë‹¤.
    """


    # 1ï¸âƒ£ ë¬¸ë‹¨ ë‚˜ëˆ„ê¸° (BERT ê¸°ë°˜)
    paragraphs = split_paragraphs_bert(sample_news)
    print("=======================1ï¸âƒ£ ë¬¸ë‹¨ ë‚˜ëˆ„ê¸° (BERT ê¸°ë°˜)========================")
    for i, para in enumerate(paragraphs):
        print(f"ë¬¸ë‹¨ {i+1}: {para}")
        print("ë¬¸ë‹¨ ê¸¸ì´ : ", len(para))

    # 2ï¸âƒ£ ë„ˆë¬´ ì§§ì€ ë¬¸ë‹¨ì„ ì• ë¬¸ë‹¨ê³¼ í•©ì¹˜ê¸°
    paragraphs = merge_short_paragraphs(paragraphs, min_length=10)
    print("=======================2ï¸âƒ£ ë„ˆë¬´ ì§§ì€ ë¬¸ë‹¨ì„ ì• ë¬¸ë‹¨ê³¼ í•©ì¹˜ê¸°========================")
    for i, para in enumerate(paragraphs):
        print(f"ë¬¸ë‹¨ {i+1}: {para}\n")

    # 3ï¸âƒ£ ë¬¸ë§¥ì´ ìœ ì‚¬í•œ ë¬¸ë‹¨ í•©ì¹˜ê¸° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜)
    merged_paragraphs = merge_based_on_similarity(paragraphs, threshold=0.9)

    # 4ï¸âƒ£ ìµœì¢… ì¶œë ¥
    print("=======================4ï¸âƒ£ ìµœì¢… ì¶œë ¥========================")
    for i, para in enumerate(merged_paragraphs):
        print(f"ë¬¸ë‹¨ {i+1}: {para}\n")
