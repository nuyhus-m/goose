from transformers import BertTokenizer, BertModel
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from soynlp.tokenizer import RegexTokenizer  # âœ… KoNLPy ëŒ€ì‹  soynlp ì‚¬ìš©
import os

# âœ… Sentence-BERT ëª¨ë¸ ë¡œë“œ
sbert_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# âœ… í˜•íƒœì†Œ ë¶„ì„ê¸° ë³€ê²½ (soynlp)
tokenizer = RegexTokenizer()

def is_connection_word(sentence):
    """
    ë¬¸ì¥ì´ ì ‘ì† ë¶€ì‚¬(í˜¹ì€ ì—°ê²°ì–´)ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸ (soynlp ê¸°ë°˜)
    """
    connection_words = {"íŠ¹íˆ", "ê·¸ëŸ¬ë‚˜", "ê²Œë‹¤ê°€", "ë˜í•œ", "ì´ì™€ í•¨ê»˜", "í•œí¸", "ë”ë¶ˆì–´", "ì´ì— ë”°ë¼", "ê²°ê³¼ì ìœ¼ë¡œ", "ì¦‰", "ì´ëŠ”"}
    words = tokenizer.tokenize(sentence)
    
    if words and words[0] in connection_words:
        return True
    return False

def split_sentences_by_period(text):
    """
    ë¬¸ì¥ì„ ì˜¨ì ('.')ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ  ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    """
    sentences = [s.strip() for s in text.split('.') if s.strip()]
    return [s + '.' for s in sentences]  # ê° ë¬¸ì¥ ëì— '.'ì„ ìœ ì§€

def get_sentence_embeddings(text):
    """
    KoBERTë¥¼ í™œìš©í•˜ì—¬ ë¬¸ì¥ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ì–»ìŒ.
    """
    tokenizer = BertTokenizer.from_pretrained("monologg/kobert")
    model = BertModel.from_pretrained("monologg/kobert")

    sentences = split_sentences_by_period(text)  # ğŸ”¹ ì˜¨ì  ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë¶„ë¦¬
    embeddings = []

    for sent in sentences:
        tokens = tokenizer(sent, return_tensors="pt", padding=True, truncation=True)
        with torch.no_grad():
            outputs = model(**tokens)
        embeddings.append(outputs.last_hidden_state.mean(dim=1).numpy())

    return sentences, np.array(embeddings).squeeze()

def split_paragraphs_bert(text, threshold=0.9):
    """
    ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ì—¬ ë¬¸ë‹¨ì„ êµ¬ë¶„.
    """
    sentences, embeddings = get_sentence_embeddings(text)
    similarities = cosine_similarity(embeddings)

    paragraphs = []
    current_paragraph = [sentences[0]]

    for i in range(1, len(sentences)):
        if similarities[i - 1][i] < threshold:  # ìœ ì‚¬ë„ê°€ ë‚®ìœ¼ë©´ ìƒˆë¡œìš´ ë¬¸ë‹¨ ì‹œì‘
            paragraphs.append(" ".join(current_paragraph))
            current_paragraph = [sentences[i]]
        else:
            current_paragraph.append(sentences[i])

    if current_paragraph:
        paragraphs.append(" ".join(current_paragraph))

    return paragraphs

def merge_short_paragraphs(paragraphs, min_length=15):
    """
    ë„ˆë¬´ ì§§ì€ ë¬¸ë‹¨ì„ ì• ë¬¸ë‹¨ê³¼ í•©ì¹˜ëŠ” í•¨ìˆ˜
    """
    fixed_paragraphs = []
    current_paragraph = ""

    for paragraph in paragraphs:
        if len(paragraph) < min_length and fixed_paragraphs:
            # í˜„ì¬ ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì´ì „ ë¬¸ì¥ê³¼ í•©ì¹¨
            fixed_paragraphs[-1] += " " + paragraph
        else:
            fixed_paragraphs.append(paragraph)

    return fixed_paragraphs

def merge_based_on_similarity(paragraphs, threshold=0.85):
    """
    ë¬¸ë§¥ì´ ìœ ì‚¬í•œ ë¬¸ë‹¨ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ëŠ” í•¨ìˆ˜ (Sentence-BERT ê¸°ë°˜)
    """
    if len(paragraphs) < 2:
        return paragraphs  # ë³‘í•©í•  ë¬¸ë‹¨ì´ í•˜ë‚˜ë¿ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

    vectors = sbert_model.encode(paragraphs)  # âœ… SBERT ë¬¸ì¥ ì„ë² ë”© ìƒì„±

    merged_paragraphs = [paragraphs[0]]

    for i in range(1, len(paragraphs)):
        similarity = cosine_similarity([vectors[i - 1]], [vectors[i]])[0][0]

        # âœ… ìœ ì‚¬ë„ê°€ ë†’ê±°ë‚˜, ë¬¸ì¥ì´ ì ‘ì† ë¶€ì‚¬ë¡œ ì‹œì‘í•˜ë©´ ë³‘í•©
        if similarity > threshold or is_connection_word(paragraphs[i]):
            merged_paragraphs[-1] += " " + paragraphs[i]
        else:
            merged_paragraphs.append(paragraphs[i])

    return merged_paragraphs

def merge_based_on_naturalness(paragraphs, threshold=0.85):
    """
    ë¬¸ë§¥ì  ìì—°ìŠ¤ëŸ¬ì›€ì„ ê³ ë ¤í•˜ì—¬ ë¬¸ì¥ì„ ë³‘í•©í•˜ëŠ” í•¨ìˆ˜
    """
    if len(paragraphs) < 2:
        return paragraphs

    merged_paragraphs = [paragraphs[0]]

    for i in range(1, len(paragraphs)):
        # âœ… ì• ë¬¸ì¥ê³¼ í˜„ì¬ ë¬¸ì¥ì„ ì—°ê²°í–ˆì„ ë•Œì˜ ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€
        combined_text = merged_paragraphs[-1] + " " + paragraphs[i]
        
        # âœ… ë‘ ê°œì˜ ë¬¸ì¥ ë¬¶ìŒì˜ SBERT ë²¡í„° ìƒì„±
        original_vector = sbert_model.encode([paragraphs[i]])
        combined_vector = sbert_model.encode([combined_text])
        
        # âœ… ìœ ì‚¬ë„ ê³„ì‚°
        similarity = cosine_similarity(original_vector, combined_vector)[0][0]

        if similarity > threshold:  # ì—°ê²°ì´ ìì—°ìŠ¤ëŸ½ë‹¤ë©´ ë³‘í•©
            merged_paragraphs[-1] += " " + paragraphs[i]
        else:
            merged_paragraphs.append(paragraphs[i])

    return merged_paragraphs

def final_merge_paragraphs(paragraphs, threshold=0.85, min_length=15):
    """
    ë¬¸ë§¥ì  ìœ ì‚¬ë„, ë¬¸ì¥ ìì—°ìŠ¤ëŸ¬ì›€, ë¬¸ì¥ ê¸¸ì´ë¥¼ ê³ ë ¤í•œ ìµœì ì˜ ë³‘í•© ë°©ì‹
    """
    # 1ï¸âƒ£ ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì• ë¬¸ì¥ê³¼ ë³‘í•©
    paragraphs = merge_short_paragraphs(paragraphs, min_length=min_length)

    # 2ï¸âƒ£ ë¬¸ì¥ì´ ì—°ê²°ì–´ë¡œ ì‹œì‘í•˜ë©´ ì• ë¬¸ì¥ê³¼ ë³‘í•©
    paragraphs = merge_based_on_similarity(paragraphs, threshold=threshold)

    # 3ï¸âƒ£ ë¬¸ì¥ì´ ë¬¸ë§¥ì ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ë‹¤ë©´ ì• ë¬¸ì¥ê³¼ ë³‘í•©
    paragraphs = merge_based_on_naturalness(paragraphs, threshold=threshold)

    return paragraphs

def load_text_file(file_path):
    """
    ì£¼ì–´ì§„ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì½ì–´ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            return file.read().strip()  # ì•ë’¤ ê³µë°± ì œê±°
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return ""

if __name__ == "__main__":
    # ğŸ”¹ news.txt íŒŒì¼ì—ì„œ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
    sample_news = load_text_file("news.txt")

    if not sample_news:
        print("âŒ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        paragraphs = split_paragraphs_bert(sample_news)
        paragraphs = final_merge_paragraphs(paragraphs, threshold=0.85, min_length=15)

        print("======================= ìµœì¢… ì¶œë ¥ ========================")
        for i, para in enumerate(paragraphs):
            print(f"ë¬¸ë‹¨ {i+1}: {para}\n")
