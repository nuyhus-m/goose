from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from transformers import (
    BartForConditionalGeneration, PreTrainedTokenizerFast,
    AutoModelForSequenceClassification, AutoTokenizer
)

app = FastAPI()

# âœ… GPU ë˜ëŠ” CPU ìë™ ê°ì§€
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… 1. KoBART ëª¨ë¸ ë¡œë“œ (í…ìŠ¤íŠ¸ ìš”ì•½)
kobart_model = BartForConditionalGeneration.from_pretrained("digit82/kobart-summarization").to(device)
kobart_tokenizer = PreTrainedTokenizerFast.from_pretrained("digit82/kobart-summarization")

# âœ… 2. KLUE-RoBERTa Large ëª¨ë¸ ë¡œë“œ (ìì—°ì–´ ì¶”ë¡ )
nli_model = AutoModelForSequenceClassification.from_pretrained("klue/roberta-large").to(device)
nli_tokenizer = AutoTokenizer.from_pretrained("klue/roberta-large")

class FactCheckRequest(BaseModel):
    content: str
    referenceContent: str

@app.post("/title-compare-contents")
async def fact_check(request: FactCheckRequest):
    try:        
        content = request.content
        referenceContent = request.referenceContent

        # âœ… 1. ë‰´ìŠ¤ ë³¸ë¬¸ ìš”ì•½ (KoBART)
        inputs_main = kobart_tokenizer.encode("summarize: " + content, return_tensors="pt", max_length=300, truncation=True).to(device)
        with torch.no_grad():
            summary_ids_main = kobart_model.generate(inputs_main, max_length=300, num_beams=4, early_stopping=True)
        summary_main = kobart_tokenizer.decode(summary_ids_main[0], skip_special_tokens=True)
        
        print("ğŸ”¹ [ìš”ì•½ëœ ë‰´ìŠ¤ ë‚´ìš©] :", summary_main)

        # âœ… 2. ë ˆí¼ëŸ°ìŠ¤ ë‰´ìŠ¤ ë³¸ë¬¸ ìš”ì•½ (KoBART)
        inputs_reference = kobart_tokenizer.encode("summarize: " + referenceContent, return_tensors="pt", max_length=300, truncation=True).to(device)
        with torch.no_grad():
            summary_ids_reference = kobart_model.generate(inputs_reference, max_length=300, num_beams=4, early_stopping=True)
        summary_reference = kobart_tokenizer.decode(summary_ids_reference[0], skip_special_tokens=True)
        
        print("ğŸ”¹ [ìš”ì•½ëœ ë ˆí¼ëŸ°ìŠ¤ ë‰´ìŠ¤ ë‚´ìš©] :", summary_reference)

        # âœ… 3. ìì—°ì–´ ì¶”ë¡  (íŒ©íŠ¸ ê²€ì¦)
        premise = summary_reference  # ìš”ì•½ëœ ë ˆí¼ëŸ°ìŠ¤ ë³¸ë¬¸ (ì „ì œ)
        hypothesis = summary_main  # ìš”ì•½ëœ ë©”ì¸ ë‰´ìŠ¤ ë³¸ë¬¸ (ê°€ì„¤)

        # âœ… 4. ì…ë ¥ ë°ì´í„° ë³€í™˜ ë° ì˜ˆì¸¡ (ë©”ëª¨ë¦¬ ìµœì í™”)
        with torch.no_grad():
            encoded_input = nli_tokenizer(premise, hypothesis, return_tensors="pt", truncation=True, padding=True).to(device)
            output = nli_model(**encoded_input)
            probs = torch.nn.functional.softmax(output.logits, dim=1)

        # âœ… 5. í™•ë¥ ê°’ ê¸°ë°˜ íŒë³„ ë¡œì§ ê°œì„ 
        num_classes = probs.shape[1]  # ëª¨ë¸ì˜ ì‹¤ì œ ì¶œë ¥ í¬ê¸°
        entailment_prob = probs[0][0].item()  # True í™•ë¥ 
        neutral_prob = probs[0][1].item() if num_classes == 3 else 0.5  # 2-class ëª¨ë¸ ëŒ€ë¹„
        contradiction_prob = probs[0][2].item() if num_classes == 3 else 1 - entailment_prob  # 2-class ëª¨ë¸ ëŒ€ë¹„

        print(f"ğŸ”¹ [ìœ ì‚¬ë„] : {entailment_prob:.4f}")        

        return {"similarity_score": entailment_prob}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
