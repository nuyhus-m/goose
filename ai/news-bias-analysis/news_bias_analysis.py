#!/usr/bin/env python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import numpy as np
import torch
import os

app = FastAPI()

device = "cuda" if torch.cuda.is_available() else "cpu"

# SentenceTransformer ëª¨ë¸ ì´ˆê¸°í™” (ì„ë² ë”© ê³„ì‚°ìš©)
embedding_model = SentenceTransformer("all-MiniLM-L6-v2", device=device)

# ChromaDB ì—°ê²° ì •ë³´ (0.4.24 ë²„ì „ìš©)
CHROMA_HOST = "i12d208.p.ssafy.io"
CHROMA_PORT = 8001

# ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± (0.4.24 ë°©ì‹)
chroma_client = chromadb.HttpClient(
    host=CHROMA_HOST,
    port=CHROMA_PORT,
    settings=Settings(
        anonymized_telemetry=False  # 0.4.24ì—ì„œëŠ” ì´ ì„¤ì •ì´ í•„ìš”í•  ìˆ˜ë„ ìˆìŒ
    )
)

# ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (0.4.24ì—ì„œëŠ” ì°¨ì› ëª…ì‹œ ê°€ëŠ¥)
reference_news_chroma_collection_paragraphs = chroma_client.get_or_create_collection(
    name="reference_paragraphs_v2"
)
reference_news_chroma_collection_content = chroma_client.get_or_create_collection(
    name="reference_content_v2"
)
news_articles_chroma_collection_title = chroma_client.get_or_create_collection(
    name="news_articles_title_v2"
)
news_articles_chroma_collection_content = chroma_client.get_or_create_collection(
    name="news_articles_content_v2"
)
news_articles_chroma_collection_paragraphs = chroma_client.get_or_create_collection(
    name="news_articles_paragraphs_v2"
)

# Pydantic ëª¨ë¸ ì •ì˜
class TitleAnalysisRequest(BaseModel):
    newsId: str
    referenceNewsId: str


class ParagraphAnalysisRequest(BaseModel):
    newsId: str
    referenceNewsId: str
    paragraphIndices: Optional[List[int]] = None


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
def compute_cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    return float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))


# ğŸ”¹ ì œëª©ê³¼ ì°¸ì¡° ë‰´ìŠ¤ ë‚´ìš© ë¹„êµ
@app.post("/title-compare-contents")
async def title_bias_analyse(request: TitleAnalysisRequest):
    try:
        news_id = f"{request.newsId}_title"
        ref_id = f"{request.referenceNewsId}_content"

        print(f"ğŸ” ì¡°íšŒí•˜ë ¤ëŠ” ë‰´ìŠ¤ ID: {news_id}")
        print(f"ğŸ” ì¡°íšŒí•˜ë ¤ëŠ” ì°¸ì¡° ë‰´ìŠ¤ ID: {ref_id}")

        title_result = news_articles_chroma_collection_title.get(ids=[news_id], include=["embeddings", "documents"])
        ref_result = reference_news_chroma_collection_content.get(ids=[ref_id], include=["embeddings", "documents"])

        print(f"ğŸ” ì œëª© ë°ì´í„° ê²°ê³¼: {title_result}")
        print(f"ğŸ” ì°¸ì¡° ë‰´ìŠ¤ ë°ì´í„° ê²°ê³¼: {ref_result}")

        if not title_result["ids"] or title_result["embeddings"][0] is None:
            raise HTTPException(status_code=404, detail=f"ë‰´ìŠ¤ ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤: {news_id}")

        if not ref_result["ids"] or ref_result["embeddings"][0] is None:
            raise HTTPException(status_code=404, detail=f"ë ˆí¼ëŸ°ìŠ¤ ë‰´ìŠ¤ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤: {ref_id}")

        news_title = title_result["documents"][0]
        title_embedding = np.array(title_result["embeddings"][0])

        ref_content = ref_result["documents"][0]
        ref_embedding = np.array(ref_result["embeddings"][0])

        similarity = compute_cosine_similarity(title_embedding, ref_embedding)

        return {
            "title": news_title,
            "reference_content": ref_content,
            "similarity_score": similarity
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ğŸ”¹ ë¬¸ë‹¨ ê°„ ìœ ì‚¬ë„ ë¹„êµ
@app.post("/paragraph-compare-contents")
async def paragraph_bias_analyse(request: ParagraphAnalysisRequest):
    try:
        if request.paragraphIndices:
            query_ids = [f"{request.newsId}_p_{idx}" for idx in request.paragraphIndices]
        else:
            raise HTTPException(status_code=400, detail="ë¬¸ë‹¨ ì¸ë±ìŠ¤ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.")

        news_paragraphs = news_articles_chroma_collection_paragraphs.get(ids=query_ids, include=["embeddings", "documents"])

        ref_paragraph_ids = [f"{request.referenceNewsId}_p_{idx}" for idx in range(10)]
        ref_paragraphs = reference_news_chroma_collection_paragraphs.get(ids=ref_paragraph_ids, include=["embeddings", "documents"])

        if not news_paragraphs["ids"] or any(e is None for e in news_paragraphs["embeddings"]):
            raise HTTPException(status_code=404, detail=f"ë‰´ìŠ¤ ê¸°ì‚¬ ë¬¸ë‹¨ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤: {query_ids}")

        if not ref_paragraphs["ids"] or any(e is None for e in ref_paragraphs["embeddings"]):
            raise HTTPException(status_code=404, detail=f"ë ˆí¼ëŸ°ìŠ¤ ë‰´ìŠ¤ ë¬¸ë‹¨ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤: {ref_paragraph_ids}")

        results = []
        for i, news_paragraph in enumerate(news_paragraphs["documents"]):
            news_embedding = np.array(news_paragraphs["embeddings"][i])

            similarities = [
                compute_cosine_similarity(news_embedding, np.array(ref_embedding))
                for ref_embedding in ref_paragraphs["embeddings"]
            ]

            max_similarity = max(similarities) if similarities else 0.0
            results.append({
                "paragraph_index": request.paragraphIndices[i],
                "similarity": max_similarity
            })

        return {"similarities": results}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
