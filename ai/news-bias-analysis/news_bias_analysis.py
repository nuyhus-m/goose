#!/usr/bin/env python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import chromadb
from sentence_transformers import SentenceTransformer
import numpy as np
import torch
import os
import openai

# OpenAI API Key ì„¤ì • (ì‹¤ì œ í‚¤ë¡œ êµì²´)
openai.api_key = os.environ.get("OPENAI_API_KEY", "")

app = FastAPI()

device = "cuda" if torch.cuda.is_available() else "cpu"


# SentenceTransformer ëª¨ë¸ ì´ˆê¸°í™” (ì„ë² ë”© ê³„ì‚°ìš©)
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# ChromaDB ì—°ê²° ì •ë³´
CHROMA_HOST = "i12d208.p.ssafy.io"
CHROMA_PORT = 8000

# ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
chroma_client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)

# ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
reference_news_chroma_collection_paragraphs = chroma_client.get_or_create_collection(name="reference_paragraphs")
reference_news_chroma_collection_content = chroma_client.get_or_create_collection(name="reference_content")
news_articles_chroma_collection_title = chroma_client.get_or_create_collection(name="news_articles_title")
news_articles_chroma_collection_content = chroma_client.get_or_create_collection(name="news_articles_content")
news_articles_chroma_collection_paragraphs = chroma_client.get_or_create_collection(name="news_articles_paragraphs")

# Pydantic ëª¨ë¸ ì •ì˜
class TitleAnalysisRequest(BaseModel):
    newsId: str
    referenceNewsId: str

class ParagraphAnalysisRequest(BaseModel):
    newsId: str
    referenceNewsId: str
    paragraphIndices: Optional[List[int]] = None  # íŠ¹ì • ë¬¸ë‹¨ë§Œ ë¹„êµí•  ìˆ˜ë„ ìˆìŒ

class NewsAnalysisRequest(BaseModel):
    newsId: str
    referenceNewsId: str


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
def compute_cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    return float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))


# ğŸ”¹ ì œëª©ê³¼ ì°¸ì¡° ë‰´ìŠ¤ ë‚´ìš© ë¹„êµ
@app.post("/title-compare-contents")
async def title_bias_analyse(request: TitleAnalysisRequest):
    try:
        # ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª© ì¡°íšŒ
        title_query = news_articles_chroma_collection_title.query(
            query_texts=[request.newsId], n_results=1, include=["documents", "embeddings"]
        )
        if not title_query["documents"] or not title_query["documents"][0]:
            raise HTTPException(status_code=404, detail="ë‰´ìŠ¤ ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

        news_title = title_query["documents"][0][0]
        title_embedding = np.array(title_query["embeddings"][0][0])

        # ë ˆí¼ëŸ°ìŠ¤ ë‰´ìŠ¤ ì „ì²´ ë‚´ìš© ì¡°íšŒ
        ref_query = reference_news_chroma_collection_content.query(
            query_texts=[request.referenceNewsId], n_results=1, include=["documents", "embeddings"]
        )
        if not ref_query["documents"] or not ref_query["documents"][0]:
            raise HTTPException(status_code=404, detail="ë ˆí¼ëŸ°ìŠ¤ ë‰´ìŠ¤ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

        ref_content = ref_query["documents"][0][0]
        ref_embedding = np.array(ref_query["embeddings"][0][0])

        # ìœ ì‚¬ë„ ê³„ì‚°
        similarity = compute_cosine_similarity(title_embedding, ref_embedding)

        return {"title": news_title, "reference_content": ref_content, "similarity": similarity}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ğŸ”¹ ë¬¸ë‹¨ ê°„ ìœ ì‚¬ë„ ë¹„êµ
@app.post("/paragraph-compare-contents")
async def paragraph_bias_analyse(request: ParagraphAnalysisRequest):
    try:
        # ë‰´ìŠ¤ ê¸°ì‚¬ ë¬¸ë‹¨ ì¡°íšŒ
        if request.paragraphIndices:
            query_ids = [f"{request.newsId}_p_{idx}" for idx in request.paragraphIndices]
        else:
            query_ids = [f"{request.newsId}_p_{idx}" for idx in range(10)]  # ì˜ˆì œ: ìµœëŒ€ 10ê°œ ë¬¸ë‹¨ ê°€ì ¸ì˜¤ê¸°

        news_paragraphs = news_articles_chroma_collection_paragraphs.get(query_ids=query_ids)
        if not news_paragraphs or not news_paragraphs["documents"]:
            raise HTTPException(status_code=404, detail="ë‰´ìŠ¤ ê¸°ì‚¬ ë¬¸ë‹¨ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

        # ì°¸ì¡° ë‰´ìŠ¤ ë¬¸ë‹¨ ì¡°íšŒ
        ref_paragraphs = reference_news_chroma_collection_paragraphs.get(query_ids=[request.referenceNewsId])
        if not ref_paragraphs or not ref_paragraphs["documents"]:
            raise HTTPException(status_code=404, detail="ë ˆí¼ëŸ°ìŠ¤ ë‰´ìŠ¤ ë¬¸ë‹¨ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

        results = []
        for i, news_paragraph in enumerate(news_paragraphs["documents"]):
            news_embedding = np.array(news_paragraphs["embeddings"][i])

            # ê° ë¬¸ë‹¨ê³¼ ì°¸ì¡° ë¬¸ë‹¨ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = [
                compute_cosine_similarity(news_embedding, np.array(ref_embedding))
                for ref_embedding in ref_paragraphs["embeddings"]
            ]

            max_similarity = max(similarities) if similarities else 0.0
            results.append({"paragraph_index": request.paragraphIndices[i], "similarity": max_similarity})

        return {"similarities": results}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ğŸ”¹ ë‰´ìŠ¤ ì „ì²´ ë‚´ìš© ìœ ì‚¬ë„ ë¹„êµ
@app.post("/news-compare")
async def news_bias_analyse(request: NewsAnalysisRequest):
    try:
        # ë‰´ìŠ¤ ê¸°ì‚¬ ì „ì²´ ë‚´ìš© ì¡°íšŒ
        news_query = news_articles_chroma_collection_content.query(
            query_texts=[request.newsId], n_results=1, include=["documents", "embeddings"]
        )
        if not news_query["documents"] or not news_query["documents"][0]:
            raise HTTPException(status_code=404, detail="ë‰´ìŠ¤ ê¸°ì‚¬ ì „ì²´ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

        news_content = news_query["documents"][0][0]
        news_embedding = np.array(news_query["embeddings"][0][0])

        # ë ˆí¼ëŸ°ìŠ¤ ë‰´ìŠ¤ ì „ì²´ ë‚´ìš© ì¡°íšŒ
        ref_query = reference_news_chroma_collection_content.query(
            query_texts=[request.referenceNewsId], n_results=1, include=["documents", "embeddings"]
        )
        if not ref_query["documents"] or not ref_query["documents"][0]:
            raise HTTPException(status_code=404, detail="ë ˆí¼ëŸ°ìŠ¤ ë‰´ìŠ¤ ì „ì²´ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

        ref_content = ref_query["documents"][0][0]
        ref_embedding = np.array(ref_query["embeddings"][0][0])

        # ìœ ì‚¬ë„ ê³„ì‚°
        similarity = compute_cosine_similarity(news_embedding, ref_embedding)

        return {"news_content": news_content, "reference_content": ref_content, "similarity": similarity}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
